# ============================================================================
# Docker Compose override for GNN Service + RAG + Ollama
# ============================================================================
version: '3.8'

services:
  # Build base GPU image
  base-ai-gpu:
    build:
      context: .
      dockerfile: docker/base-ai-gpu.Dockerfile
    image: hydraulic-ai-base-gpu:cuda12.1
    profiles: ["build"]

  # GNN Service (T-GAT)
  gnn_service:
    build:
      context: ./gnn_service
      dockerfile: Dockerfile
    image: hydraulic-gnn-service:tgat
    container_name: hdx-gnn-service
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - GNN_INTERNAL_API_KEY=${GNN_INTERNAL_API_KEY}
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/hydraulic_db
      - REDIS_URL=redis://redis:6379/0
    volumes:
      - ./gnn_service/models:/app/models
      - ./gnn_service/checkpoints:/app/checkpoints
    networks:
      - internal
    depends_on:
      - postgres
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 60s

  # Ollama (GPU for RAG)
  ollama:
    image: ollama/ollama:latest
    container_name: hdx-ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ollama_models:/root/.ollama
    networks:
      - internal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Ollama init (pull DeepSeek-R1)
  ollama-init:
    image: ollama/ollama:latest
    container_name: hdx-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama_models:/root/.ollama
    networks:
      - internal
    environment:
      - OLLAMA_HOST=http://ollama:11434
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "Pulling DeepSeek-R1..."
        sleep 10
        ollama pull deepseek-r1:7b
        echo "âœ… DeepSeek-R1 ready"
    restart: "no"

  # RAG Service
  rag_service:
    depends_on:
      ollama-init:
        condition: service_completed_successfully
    environment:
      - RAG_OLLAMA_BASE_URL=http://ollama:11434
      - RAG_LLM_MODEL=deepseek-r1:7b

  # Backend updates
  backend:
    depends_on:
      gnn_service:
        condition: service_healthy
      rag_service:
        condition: service_healthy

volumes:
  ollama_models:
    driver: local

networks:
  internal:
    driver: bridge
