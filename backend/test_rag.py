import os
import sys
import django
import time
from typing import List, Dict, Any
import numpy as np

from apps.rag_assistant.rag_core import RAGOrchestrator, LocalStorageBackend, EmbeddingsProvider, DEFAULT_LOCAL_STORAGE
from apps.rag_assistant.llm_factory import LLMFactory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Django Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ, ÐµÑÐ»Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº settings
BASE_DIR = os.path.dirname(__file__)
sys.path.insert(0, BASE_DIR)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "core.settings")

django.setup()


def format_docs(docs: List[Dict[str, Any]]) -> str:
    return "\n\n".join(d["content"] for d in docs if d.get("content"))


def build_rag_chain(vindex, embedder, llm):
    def _encode(texts: List[str]) -> np.ndarray:
        embs = embedder.embed_documents(texts)
        arr = np.array(embs, dtype="float32")
        norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12
        return (arr / norms).astype("float32")

    docs_list = (vindex.metadata or {}).get("docs", [])

    def retrieve(question: str):
        q_emb = _encode([question])
        _, indices = vindex.search(q_emb, k=4)
        results = []
        for i in indices[0]:
            if 0 <= i < len(docs_list):
                results.append({"content": docs_list[i], "meta": {"idx": int(i)}})
        return results

    prompt = ChatPromptTemplate.from_template(
        """
        You are a helpful assistant for hydraulic diagnostics.
        Use the following retrieved context to answer the user's question concisely and accurately.
        If the answer is not present in the context, say you don't know.

        Context:
        {context}

        Question:
        {question}

        Answer in the same language as the question.
        """
    )
    parser = StrOutputParser()

    chain = (
        {"question": RunnablePassthrough()}
        | {"docs": RunnableLambda(lambda x: retrieve(x["question"]))}
        | RunnableLambda(lambda x: {"question": x["question"], "context": format_docs(x["docs"])})
        | prompt
        | llm
        | parser
    )
    return chain


def main():
    print("âœ… AI Engine Ð¸ RAG ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹")

    # 1) Ð”Ð°Ð½Ð½Ñ‹Ðµ (Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð½Ð° ÑÐ²Ð¾Ð¸)
    docs = [
        "Hydraulic pressure is low, check pump operation and fluid levels",
        "Pressure relief valve stuck open, system cannot maintain pressure",
        "Pump failure suspected due to unusual noise and vibration patterns",
        "Filter clogged, causing cavitation and reduced flow",
        "Air in hydraulic lines can lead to spongy response and delayed actuation",
    ]
    version = "v_test_v2"

    # 2) Ð¡Ð±Ð¾Ñ€ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ° (ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ docs Ð² metadata)
    print("ðŸ“¦ Building embeddings and index via Ollama (nomic-embed-text)...")
    storage = LocalStorageBackend(base_path=os.path.abspath(DEFAULT_LOCAL_STORAGE))
    # Ð’ ÑÑ‚Ð¾Ð¼ Ñ‚ÐµÑÑ‚Ðµ embedder rag_core Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ â€” Ð²Ð¼ÐµÑÑ‚Ð¾ Ð½ÐµÐ³Ð¾ Ollama embeddings Ð¸Ð· LLMFactory
    # Ð½Ð¾ orchestrator Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ EmbeddingsProvider, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¿Ð¾Ð´ÑÑ‚Ð°Ð²Ð¸Ð¼ Ð»ÑŽÐ±Ð¾Ð¹ (Ð¾Ð½ Ð½Ðµ Ð±ÑƒÐ´ÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ Ð½Ð¸Ð¶Ðµ)
    dummy_embedder = EmbeddingsProvider(model_name="sentence-transformers/all-MiniLM-L6-v2")
    orchestrator = RAGOrchestrator(storage=storage, embedder=dummy_embedder, index_metric="ip")
    orchestrator()

    # Ð’ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ð¼ Ñ‡ÐµÑ€ÐµÐ· OllamaEmbeddings, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð½Ð´ÐµÐºÑ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ð» Ñ€ÐµÑ‚Ñ€Ð¸Ð²ÐµÑ€Ñƒ
    ollama_embedder = LLMFactory.create_embedder()
    embs = np.array(ollama_embedder.embed_documents(docs), dtype="float32")
    norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12
    embs = (embs / norms).astype("float32")

    # Ð¡Ð¾Ð±ÐµÑ€ÐµÐ¼ Ð¸Ð½Ð´ÐµÐºÑ Ð²Ñ€ÑƒÑ‡Ð½ÑƒÑŽ, Ð·Ð°Ñ‚ÐµÐ¼ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ð¼ Ñ‡ÐµÑ€ÐµÐ· orchestrator.save_index
    from apps.rag_assistant.rag_core import VectorIndex
    vindex = VectorIndex(dim=embs.shape[1], metric="ip")
    vindex.build(embs)
    index_bytes = vindex.to_bytes()
    meta = {"dim": embs.shape[1], "metric": "ip", "docs": docs}
    storage.save_index(version, index_bytes, meta)

    print(f"âœ… Index saved to: {os.path.join(DEFAULT_LOCAL_STORAGE, f'v_{version}')}")

    # 3) Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ° Ð¸ ÑÐ±Ð¾Ñ€ÐºÐ° Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸
    idx_bytes, loaded_meta = storage.load_index(version)
    vindex2 = VectorIndex.from_bytes(idx_bytes)
    vindex2.metadata = loaded_meta

    llm = LLMFactory.create_chat_model()  # qwen3:8b Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ

    chain = build_rag_chain(vindex2, ollama_embedder, llm)

    # 4) Ð—Ð°Ð¿Ñ€Ð¾ÑÑ‹
    print("ðŸ” Testing RAG chain...")
    queries = ["pump problems", "pressure issues", "air in lines"]
    for i, q in enumerate(queries, 1):
        t0 = time.time()
        answer = chain.invoke({"question": q})
        dt = time.time() - t0
        print(f"\nQuery {i} - '{q}':")
        print("Answer:", answer)
        print(f"(took {dt:.2f}s)")

    print("\nðŸŽ‰ RAG test completed successfully!")


if __name__ == "__main__":
    main()
