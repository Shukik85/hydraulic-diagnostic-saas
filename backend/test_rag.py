"""ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ñ Ð°Ð²Ñ‚Ð¾Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼ Ð´Ð¾ÐºÑÑ‚Ñ€Ð¸Ð½Ð³Ð¾Ð¼."""

import json
import os
import sys
import time
from pathlib import Path
from typing import Any, Dict, List

import django

import numpy as np
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Django Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ (Ð´Ð»Ñ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº settings, ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾)
BASE_DIR = os.path.dirname(__file__)
sys.path.insert(0, BASE_DIR)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "core.settings")

django.setup()

# ÐŸÐ¾ÑÐ»Ðµ django.setup() Ð¼Ð¾Ð¶Ð½Ð¾ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Django-Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ðµ Ð¼Ð¾Ð´ÑƒÐ»Ð¸
from apps.rag_assistant.llm_factory import LLMFactory  # noqa: E402
from apps.rag_assistant.rag_core import (  # noqa: E402
    DEFAULT_LOCAL_STORAGE,
    LocalStorageBackend,
    VectorIndex,
)


def format_docs(docs: List[Dict[str, Any]]) -> str:
    return "\n\n".join(d["content"] for d in docs if d.get("content"))


def build_rag_chain(vindex: VectorIndex, ollama_embedder, llm):
    def _encode(texts: List[str]) -> np.ndarray:
        embs = ollama_embedder.embed_documents(texts)
        arr = np.array(embs, dtype="float32")
        norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12
        return (arr / norms).astype("float32")

    docs_list = (vindex.metadata or {}).get("docs", [])

    def retrieve(question: str):
        q_emb = _encode([question])
        _, indices = vindex.search(q_emb, k=4)
        results = []
        for i in indices[0]:
            if 0 <= i < len(docs_list):
                results.append({"content": docs_list[i], "meta": {"idx": int(i)}})
        return results

    prompt = ChatPromptTemplate.from_template(
        """
        You are a helpful assistant for hydraulic diagnostics.
        Use the following retrieved context to answer the user's question concisely and accurately.
        If the answer is not present in the context, say you don't know.

        Context:
        {context}

        Question:
        {question}

        Answer in the same language as the question.
        """
    )
    parser = StrOutputParser()

    chain = (
        {"question": RunnablePassthrough()}
        | {"docs": RunnableLambda(lambda x: retrieve(x["question"]))}
        | RunnableLambda(
            lambda x: {"question": x["question"], "context": format_docs(x["docs"])}
        )
        | prompt
        | llm
        | parser
    )
    return chain


def main():
    print("âœ… AI Engine Ð¸ RAG ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹")

    # 1) Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹
    docs = [
        "Hydraulic pressure is low, check pump operation and fluid levels",
        "Pressure relief valve stuck open, system cannot maintain pressure",
        "Pump failure suspected due to unusual noise and vibration patterns",
        "Filter clogged, causing cavitation and reduced flow",
        "Air in hydraulic lines can lead to spongy response and delayed actuation",
    ]
    version = "v_test_v2"

    # 2) Ð¡Ð±Ð¾Ñ€ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ°: embeddings Ñ‡ÐµÑ€ÐµÐ· Ollama Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ retriever
    print("ðŸ“¦ Building embeddings and index via Ollama (nomic-embed-text)...")
    storage = LocalStorageBackend(base_path=Path(DEFAULT_LOCAL_STORAGE).absolute())

    # Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ñ‡ÐµÑ€ÐµÐ· OllamaEmbeddings
    ollama_embedder = LLMFactory.create_embedder()  # nomic-embed-text
    embs = np.array(ollama_embedder.embed_documents(docs), dtype="float32")
    norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12
    embs = (embs / norms).astype("float32")

    vindex = VectorIndex(dim=embs.shape[1], metric="ip")
    vindex.build(embs)
    index_bytes = vindex.to_bytes()
    meta = {"dim": embs.shape[1], "metric": "ip", "docs": docs}
    save_path = storage.save_index(version, index_bytes, meta)
    print(json.dumps({"event": "index_saved", "path": save_path}, ensure_ascii=False))

    # 3) Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ° Ð¸ ÑÐ±Ð¾Ñ€ÐºÐ° Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸
    idx_bytes, loaded_meta = storage.load_index(version)
    vindex2 = VectorIndex.from_bytes(idx_bytes)
    vindex2.metadata = loaded_meta

    llm = LLMFactory.create_chat_model()  # qwen3:8b Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ

    chain = build_rag_chain(vindex2, ollama_embedder, llm)

    # 4) Ð—Ð°Ð¿Ñ€Ð¾ÑÑ‹
    print("ðŸ” Testing RAG chain...")
    queries = ["pump problems", "pressure issues", "air in lines"]
    for _i, q in enumerate(queries, 1):
        t0 = time.time()
        answer = chain.invoke({"question": q})
        dt = time.time() - t0
        payload = {"query": q, "answer": answer, "t_sec": round(dt, 2)}
        print(json.dumps(payload, ensure_ascii=False))

    print("\nðŸŽ‰ RAG test completed successfully!")


if __name__ == "__main__":
    main()
