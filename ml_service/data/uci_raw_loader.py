#!/usr/bin/env python3
"""
UCI Hydraulic RAW Loader
- Parse raw sensor .txt files from ml_service/data/raw/uci_hydraulic
- Validate, align, feature engineer, and export processed dataset
"""
from pathlib import Path
import pandas as pd
import numpy as np
import structlog
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = structlog.get_logger()

RAW_DIR = Path("ml_service/data/raw/uci_hydraulic")
PROCESSED_PARQUET = Path("ml_service/data/processed/uci_hydraulic.parquet")
EXPORT_CSV = Path("ml_service/data/industrial_iot/Industrial_fault_detection.csv")
REPORT_PATH = Path("ml_service/reports/raw_ingestion_report.json")

SENSOR_FILES = [
    "PS1.txt","PS2.txt","PS3.txt","PS4.txt","PS5.txt","PS6.txt",
    "FS1.txt","FS2.txt",
    "TS1.txt","TS2.txt","TS3.txt","TS4.txt",
    "VS1.txt","EPS1.txt"
]

COL_MAP = {
    "PS1.txt":"PS1","PS2.txt":"PS2","PS3.txt":"PS3","PS4.txt":"PS4","PS5.txt":"PS5","PS6.txt":"PS6",
    "FS1.txt":"FS1","FS2.txt":"FS2",
    "TS1.txt":"TS1","TS2.txt":"TS2","TS3.txt":"TS3","TS4.txt":"TS4",
    "VS1.txt":"VS1","EPS1.txt":"EPS1"
}


def _read_sensor(path: Path) -> pd.Series:
    """Read single sensor file - tab-separated values in rows."""
    try:
        # Read as single column of tab-separated values
        with open(path, 'r') as f:
            values = []
            for line in f:
                line = line.strip()
                if line:
                    # Split by tab and take all numeric values
                    parts = line.split('\t')
                    for part in parts:
                        try:
                            val = float(part.strip())
                            values.append(val)
                        except ValueError:
                            continue
        return pd.Series(values)
    except Exception as e:
        logger.error(f"Failed to read {path}: {e}")
        return pd.Series([])


def load_raw_align() -> pd.DataFrame:
    """Load and align all sensor files."""
    if not RAW_DIR.exists():
        raise FileNotFoundError(f"RAW directory not found: {RAW_DIR}")
    
    missing = [f for f in SENSOR_FILES if not (RAW_DIR/f).exists()]
    if missing:
        logger.warning(f"Missing sensor files: {missing}")
        # Use only available files
        available_files = [f for f in SENSOR_FILES if (RAW_DIR/f).exists()]
        if not available_files:
            raise FileNotFoundError("No sensor files found")
    else:
        available_files = SENSOR_FILES

    logger.info(f"Loading {len(available_files)} sensor files from {RAW_DIR}")
    
    series = {}
    with ThreadPoolExecutor(max_workers=min(8, len(available_files))) as ex:
        futs = {ex.submit(_read_sensor, RAW_DIR/f): f for f in available_files}
        for fut in as_completed(futs):
            fname = futs[fut]
            s = fut.result()
            if len(s) > 0:
                series[COL_MAP[fname]] = s
                logger.info(f"Loaded {fname}: {len(s)} values")
            else:
                logger.warning(f"Empty data from {fname}")

    if not series:
        raise ValueError("No valid sensor data loaded")
    
    # Align by minimum length
    min_len = min(len(s) for s in series.values())
    logger.info(f"Aligning to minimum length: {min_len}")
    
    for k in series:
        series[k] = series[k].iloc[:min_len].reset_index(drop=True)

    df = pd.DataFrame(series)
    # Synthetic Timestamp @10 Hz (100ms intervals) - fix deprecated 'L' 
    ts = pd.date_range("2025-01-01", periods=min_len, freq="100ms")
    df.insert(0, "Timestamp", ts)
    
    logger.info(f"Created dataframe with {len(df)} rows and columns: {list(df.columns)}")
    return df


def apply_profile_labels(df: pd.DataFrame) -> pd.DataFrame:
    """Apply fault labels from profile.txt if available."""
    profile_path = RAW_DIR/"profile.txt"
    if profile_path.exists():
        try:
            logger.info(f"Loading profile from {profile_path}")
            prof = pd.read_csv(profile_path, sep="\s+", header=None, engine="python")
            
            # Handle profile length mismatch - interpolate/repeat profile to match sensor length
            if len(prof) != len(df):
                logger.warning(f"Profile length {len(prof)} != sensor length {len(df)}, interpolating...")
                
                # Create index mapping from profile to sensor data
                profile_indices = np.linspace(0, len(prof)-1, len(df))
                
                # Interpolate each profile column
                prof_aligned = pd.DataFrame()
                for col_idx in range(prof.shape[1]):
                    prof_values = prof.iloc[:, col_idx].values
                    # Use nearest neighbor interpolation for categorical data like flags
                    aligned_values = prof_values[np.round(profile_indices).astype(int)]
                    prof_aligned[col_idx] = aligned_values
                
                prof = prof_aligned
            
            prof = prof.iloc[:len(df)].reset_index(drop=True)
            
            # Assume columns: load, cooler, vib_flag, p_set, fault_flag
            vib_flag = prof.iloc[:,2] if prof.shape[1] >= 3 else pd.Series(np.zeros(len(df)))
            fault_flag = prof.iloc[:,4] if prof.shape[1] >= 5 else pd.Series(np.zeros(len(df)))
            
            # Binary fault: fault_flag>0 OR vib_flag>0
            fault_bin = ((fault_flag>0) | (vib_flag>0)).astype(int)
            
            fault_dist = fault_bin.value_counts().to_dict()
            logger.info(f"Profile labels applied: {fault_dist}")
            
        except Exception as e:
            logger.warning(f"Failed to parse profile.txt: {e}, defaulting labels to 0")
            fault_bin = pd.Series(np.zeros(len(df), dtype=int))
    else:
        logger.warning("profile.txt not found, defaulting labels to 0")
        fault_bin = pd.Series(np.zeros(len(df), dtype=int))

    df["Fault Label"] = fault_bin.values
    return df


def engineer_basic_features(df: pd.DataFrame) -> pd.DataFrame:
    """Add basic engineered features."""
    out = df.copy()
    
    # RMS for VS1 if available (1 second window = 10 samples @10Hz)
    if "VS1" in out.columns:
        out["RMS_VS1_1s"] = out["VS1"].rolling(10, min_periods=1).apply(lambda x: np.sqrt(np.mean(x**2)))
    
    # Rolling std for pressure sensors (5 second window = 50 samples)
    for col in ["PS1","PS2","PS3","PS4","PS5","PS6"]:
        if col in out.columns:
            out[f"{col}_std_5s"] = out[col].rolling(50, min_periods=1).std().fillna(0)
    
    logger.info(f"Added engineered features, total columns: {len(out.columns)}")
    return out


def export_processed(df: pd.DataFrame):
    """Export processed data to parquet and tidy CSV."""
    # Create directories
    PROCESSED_PARQUET.parent.mkdir(parents=True, exist_ok=True)
    EXPORT_CSV.parent.mkdir(parents=True, exist_ok=True)
    
    # Save full processed data as parquet
    df.to_parquet(PROCESSED_PARQUET, index=False)
    logger.info(f"Saved processed data to {PROCESSED_PARQUET}")
    
    # Create tidy CSV compatible with existing loader
    tidy_cols = {
        "Timestamp": df["Timestamp"],
        "Vibration (mm/s)": df.get("VS1", pd.Series(np.random.normal(5, 1, len(df)))),  # Fallback data
        "Temperature (¬∞C)": df.get("TS1", pd.Series(np.random.normal(60, 5, len(df)))),
        "Pressure (bar)": df.get("PS1", pd.Series(np.random.normal(160, 10, len(df)))),
        "RMS Vibration": df.get("RMS_VS1_1s", pd.Series(np.random.normal(5.2, 0.5, len(df)))),
        "Mean Temp": df.get("TS1", pd.Series(np.random.normal(60, 3, len(df)))).rolling(50, min_periods=1).mean() if "TS1" in df.columns else pd.Series(np.random.normal(60, 3, len(df))),
        "Fault Label": df["Fault Label"].astype(int)
    }
    
    tidy = pd.DataFrame(tidy_cols)
    tidy.to_csv(EXPORT_CSV, index=False)
    logger.info(f"Saved tidy CSV to {EXPORT_CSV}")
    
    return len(df)


def build_raw_dataset() -> dict:
    """Main function to build dataset from RAW files."""
    report = {"status":"ok","errors":[],"rows":0, "files_processed": []}
    
    try:
        logger.info("Starting RAW dataset build...")
        
        # Load and align sensor data
        df = load_raw_align()
        report["files_processed"] = list(df.columns[1:])  # Skip Timestamp
        
        # Apply profile labels
        df = apply_profile_labels(df)
        
        # Engineer features
        df = engineer_basic_features(df)
        
        # Export processed data
        row_count = export_processed(df)
        report["rows"] = row_count
        
        logger.info(f"‚úÖ RAW dataset build completed: {row_count} rows")
        
    except Exception as e:
        report["status"] = "failed"
        report["errors"].append(str(e))
        logger.error(f"‚ùå RAW build failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Save report
    REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
    REPORT_PATH.write_text(json.dumps(report, indent=2))
    
    return report


if __name__ == "__main__":
    import structlog
    structlog.configure(processors=[structlog.dev.ConsoleRenderer()])
    
    r = build_raw_dataset()
    print("\nüìä RAW Dataset Build Report:")
    print(json.dumps(r, indent=2))