#!/usr/bin/env python3
"""
Push UCI Hydraulic data to ML Inference API for testing

–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML —Å–µ—Ä–≤–∏—Å–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö UCI:
- –ß–∏—Ç–∞–µ—Ç cycles_sample_100.parquet
- –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ SensorDataBatch —Ñ–æ—Ä–º–∞—Ç
- –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–µ—Ä–µ–∑ /api/v1/predict/batch
- –ò–∑–º–µ—Ä—è–µ—Ç latency, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

Usage:
  # –ó–∞–ø—É—Å—Ç–∏—Ç—å ML —Å–µ—Ä–≤–∏—Å –≤ –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:
  cd ml_service && python main.py

  # –ó–∞—Ç–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:
  python ml_service/scripts/push_to_api.py --cycles 10 --api http://localhost:8001
"""

import argparse
import asyncio
import json
import time
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

import httpx
import numpy as np  # ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –¥–ª—è —Ñ–∏–∫—Å–∞ int64 serialization
import pandas as pd
import structlog

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer(),
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()


# ‚úÖ FIX: JSON serialization helper
def sanitize_for_json(obj: Any) -> Any:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç NumPy —Ç–∏–ø—ã –≤ JSON-—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: sanitize_for_json(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [sanitize_for_json(item) for item in obj]
    else:
        return obj


class MLAPITester:
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ML API —Å UCI –¥–∞–Ω–Ω—ã–º–∏."""

    def __init__(self, api_base: str, timeout: int = 30):
        self.api_base = api_base.rstrip("/")
        self.timeout = timeout
        self.results: list[dict[str, Any]] = []

    async def test_service_health(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ ML —Å–µ—Ä–≤–∏—Å–∞."""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # Health check
                health_resp = await client.get(f"{self.api_base}/health")
                ready_resp = await client.get(f"{self.api_base}/ready")

                if health_resp.status_code == 200 and ready_resp.status_code == 200:
                    logger.info("ML service is healthy and ready")
                    return True
                else:
                    logger.error(
                        "ML service not ready",
                        health_status=health_resp.status_code,
                        ready_status=ready_resp.status_code,
                    )
                    return False

        except Exception as e:
            logger.error("Failed to check ML service health", error=str(e))
            return False

    def load_uci_data(self, data_path: str, limit_cycles: int) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ UCI –¥–∞–Ω–Ω—ã—Ö –∏–∑ Parquet."""
        try:
            df = pd.read_parquet(data_path)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏–∫–ª–æ–≤
            available_cycles = df["cycle"].nunique()
            if limit_cycles > available_cycles:
                logger.warning(
                    "Requested more cycles than available", requested=limit_cycles, available=available_cycles
                )
                limit_cycles = available_cycles

            cycles_to_test = df["cycle"].unique()[:limit_cycles]
            filtered_df = df[df["cycle"].isin(cycles_to_test)]

            logger.info(
                "UCI data loaded",
                total_rows=len(filtered_df),
                cycles=limit_cycles,
                sensors=filtered_df["sensor"].nunique(),
                sensor_types=list(filtered_df["sensor_type"].unique()),
            )

            return filtered_df

        except Exception as e:
            logger.error("Failed to load UCI data", error=str(e), path=data_path)
            raise

    def convert_cycle_to_sensor_batch(self, cycle_df: pd.DataFrame, cycle_id: int) -> dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ UCI –≤ SensorDataBatch."""
        readings = []

        for _, row in cycle_df.iterrows():
            # –°–æ–∑–¥–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π timestamp –¥–ª—è —Ü–∏–∫–ª–∞
            base_time = datetime.now(UTC)
            cycle_timestamp = base_time.replace(
                second=int(row["timestamp"]) % 60,  # ‚úÖ –ü—Ä–∏–≤–æ–¥–∏–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0-59
                microsecond=int((row["timestamp"] % 1) * 1000000),
            )

            # ‚úÖ FIX: JSON-safe conversion
            reading = {
                "timestamp": cycle_timestamp.isoformat(),
                "sensor_type": str(row["sensor_type"]),
                "value": float(row["value"]) if not isinstance(row["value"], (int, float)) else float(row["value"]),
                "unit": str(row["unit"]),
                "component_id": None,  # UCI –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç component_id
            }
            readings.append(reading)

        return {
            "sensor_data": {
                "system_id": str(cycle_df["system_id"].iloc[0]),
                "readings": readings,
                "metadata": {
                    "source": "UCI_Hydraulic", 
                    "cycle": int(cycle_id), 
                    "sensors_count": int(len(readings))
                },  # ‚úÖ FIX: –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –∫ int/str
            },
            "prediction_type": "anomaly",
            "use_cache": True,
            "ml_models": ["catboost", "xgboost", "random_forest"],  # –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª–µ –∏ —Å–æ—Å—Ç–∞–≤ –º–æ–¥–µ–ª–µ–π
        }

    async def test_single_prediction(self, request_data: dict[str, Any]) -> dict[str, Any]:
        """–¢–µ—Å—Ç –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."""
        start_time = time.time()

        try:
            # ‚úÖ FIX: JSON-safe serialization
            safe_request = sanitize_for_json(request_data)
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.api_base}/api/v1/predict", 
                    json=safe_request, 
                    headers={"Content-Type": "application/json"}
                )

                request_time = (time.time() - start_time) * 1000  # ms

                if response.status_code == 200:
                    result = response.json()

                    return {
                        "success": True,
                        "request_time_ms": float(request_time),  # ‚úÖ FIX: –Ø–≤–Ω–æ–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ float
                        "api_processing_time_ms": float(result.get("total_processing_time_ms", 0)),
                        "ensemble_score": float(result.get("ensemble_score", 0)),
                        "severity": str(result.get("prediction", {}).get("severity", "unknown")),
                        "confidence": float(result.get("prediction", {}).get("confidence", 0)),
                        "models_used": int(len(result.get("ml_predictions", []))),
                        "features_extracted": int(result.get("features_extracted", 0)),
                        "cache_hit": bool(result.get("cache_hit", False)),
                    }
                else:
                    logger.error("API request failed", status_code=response.status_code, response=response.text[:200])
                    return {
                        "success": False,
                        "request_time_ms": float(request_time),
                        "error": f"HTTP {response.status_code}: {response.text[:100]}",
                    }

        except Exception as e:
            request_time = (time.time() - start_time) * 1000
            logger.error("Single prediction failed", error=str(e))
            return {"success": False, "request_time_ms": float(request_time), "error": str(e)}

    async def test_batch_prediction(self, requests_data: list[dict[str, Any]]) -> dict[str, Any]:
        """–¢–µ—Å—Ç –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."""
        start_time = time.time()

        # ‚úÖ FIX: –û–±—Ä–∞–±–æ—Ç–∫–∞ JSON serialization
        safe_requests = sanitize_for_json(requests_data)
        batch_request = {"requests": safe_requests, "parallel_processing": True}

        try:
            async with httpx.AsyncClient(timeout=self.timeout * 2) as client:
                response = await client.post(
                    f"{self.api_base}/api/v1/predict/batch",
                    json=batch_request,
                    headers={"Content-Type": "application/json"},
                )

                request_time = (time.time() - start_time) * 1000  # ms

                if response.status_code == 200:
                    result = response.json()

                    return {
                        "success": True,
                        "batch_size": int(len(requests_data)),
                        "request_time_ms": float(request_time),
                        "api_processing_time_ms": float(result.get("total_processing_time_ms", 0)),
                        "successful_predictions": int(result.get("successful_predictions", 0)),
                        "failed_predictions": int(result.get("failed_predictions", 0)),
                        "results": result.get("results", []),
                    }
                else:
                    return {
                        "success": False,
                        "batch_size": int(len(requests_data)),
                        "request_time_ms": float(request_time),
                        "error": f"HTTP {response.status_code}: {response.text[:100]}",
                    }

        except Exception as e:
            request_time = (time.time() - start_time) * 1000
            logger.error("Batch prediction failed", error=str(e))
            return {
                "success": False,
                "batch_size": int(len(requests_data)),
                "request_time_ms": float(request_time),
                "error": str(e),
            }

    def analyze_results(self, results: list[dict[str, Any]]) -> dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        if not results:
            return {"error": "No results to analyze", "total_tests": 0, "failed_tests": 0}  # ‚úÖ FIX: –î–æ–±–∞–≤–ª–µ–Ω failed_tests

        successful = [r for r in results if r.get("success", False)]
        failed = [r for r in results if not r.get("success", False)]

        if not successful:
            return {
                "total_tests": len(results),
                "successful_tests": 0,
                "failed_tests": len(failed),  # ‚úÖ FIX
                "success_rate": 0.0,
                "errors": [r.get("error", "Unknown error") for r in failed],
            }

        # –ê–Ω–∞–ª–∏–∑ latency
        request_times = [r["request_time_ms"] for r in successful]
        api_times = [r.get("api_processing_time_ms", 0) for r in successful if r.get("api_processing_time_ms")]

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        scores = [r.get("ensemble_score", 0) for r in successful if r.get("ensemble_score") is not None]
        severities = [r.get("severity", "unknown") for r in successful]
        confidences = [r.get("confidence", 0) for r in successful if r.get("confidence")]

        severity_counts: dict[str, int] = {}
        for sev in severities:
            severity_counts[sev] = severity_counts.get(sev, 0) + 1

        # ‚úÖ FIX: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        analysis = {
            "total_tests": int(len(results)),
            "successful_tests": int(len(successful)),
            "failed_tests": int(len(failed)),  # ‚úÖ FIX: –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ!
            "success_rate": float(len(successful) / len(results) * 100),
        }
        
        # Latency stats (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ)
        if request_times:
            analysis["latency_stats"] = {
                "request_time_p50_ms": float(np.percentile(request_times, 50)),
                "request_time_p90_ms": float(np.percentile(request_times, 90)),
                "request_time_p99_ms": float(np.percentile(request_times, 99)),
                "api_time_p90_ms": float(np.percentile(api_times, 90)) if api_times else 0.0,
                "target_p90_ms": 100,
                "meets_target": bool(np.percentile(request_times, 90) < 100),
            }
        
        # Prediction stats (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ)
        if scores:
            analysis["prediction_stats"] = {
                "ensemble_score_mean": float(np.mean(scores)),
                "ensemble_score_std": float(np.std(scores)),
                "confidence_mean": float(np.mean(confidences)) if confidences else 0.0,
                "severity_distribution": {str(k): int(v) for k, v in severity_counts.items()},
            }
        
        # –û—à–∏–±–∫–∏
        if failed:
            analysis["errors"] = [str(r.get("error", "Unknown")) for r in failed]

        return analysis


async def main():
    parser = argparse.ArgumentParser(description="Test ML API with UCI data")
    parser.add_argument("--cycles", type=int, default=10, help="Number of cycles to test")
    parser.add_argument("--api", type=str, default="http://localhost:8001", help="ML API base URL")
    parser.add_argument(
        "--data", type=str, default="data/uci_hydraulic/cycles_sample_100.parquet", help="UCI data file"
    )
    parser.add_argument("--batch", action="store_true", help="Use batch API instead of single predictions")
    parser.add_argument("--report", type=str, default="reports/uci_test_report.json", help="Save report to file")

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    data_path = Path(args.data)
    if not data_path.exists():
        logger.error("UCI data file not found", path=str(data_path))
        logger.info("Run first: python ml_service/scripts/fetch_uci_hydraulic.py --limit 100")
        return

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–µ—Ä
    tester = MLAPITester(args.api)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞
    if not await tester.test_service_health():
        logger.error("ML service is not ready. Start it with: cd ml_service && python main.py")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    logger.info("Loading UCI data", path=str(data_path), cycles=args.cycles)
    df = tester.load_uci_data(str(data_path), args.cycles)

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –ø–æ —Ü–∏–∫–ª–∞–º
    requests_data: list[dict[str, Any]] = []

    for cycle_id in df["cycle"].unique()[: args.cycles]:
        cycle_df = df[df["cycle"] == cycle_id]
        request = tester.convert_cycle_to_sensor_batch(cycle_df, int(cycle_id))  # ‚úÖ FIX: int()
        requests_data.append(request)

    logger.info("Prepared requests", count=len(requests_data))

    # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    results: list[dict[str, Any]] = []

    if args.batch:
        logger.info("Running batch prediction test")
        batch_result = await tester.test_batch_prediction(requests_data)
        results.append(batch_result)

        # –ï—Å–ª–∏ batch —É—Å–ø–µ—à–µ–Ω, –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if batch_result.get("success") and batch_result.get("results"):
            for individual_result in batch_result["results"]:
                if isinstance(individual_result, dict) and "ensemble_score" in individual_result:
                    results.append(
                        {
                            "success": True,
                            "request_time_ms": float(batch_result["request_time_ms"] / len(requests_data)),
                            "api_processing_time_ms": float(individual_result.get("total_processing_time_ms", 0)),
                            "ensemble_score": float(individual_result.get("ensemble_score", 0)),
                            "severity": str(individual_result.get("prediction", {}).get("severity", "unknown")),
                            "confidence": float(individual_result.get("prediction", {}).get("confidence", 0)),
                            "models_used": int(len(individual_result.get("ml_predictions", []))),
                            "features_extracted": int(individual_result.get("features_extracted", 0)),
                            "cache_hit": bool(individual_result.get("cache_hit", False)),
                        }
                    )
    else:
        logger.info("Running individual prediction tests")
        for i, request in enumerate(requests_data):
            logger.info(f"Testing cycle {i + 1}/{len(requests_data)}")
            result = await tester.test_single_prediction(request)
            results.append(result)

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(0.1)

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("Analyzing results")
    analysis = tester.analyze_results(results)

    # –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
    print("\n" + "=" * 60)
    print("üß™ UCI HYDRAULIC ML API TEST REPORT")
    print("=" * 60)

    print("\nüìä SUMMARY:")
    print(f"  Total Tests: {analysis['total_tests']}")
    print(f"  Success Rate: {analysis['success_rate']:.1f}%")
    print(f"  Failed Tests: {analysis['failed_tests']}")

    if analysis.get("latency_stats"):
        latency = analysis["latency_stats"]
        print("\n‚ö° PERFORMANCE:")
        print(f"  Request Time p50: {latency['request_time_p50_ms']:.1f}ms")
        print(f"  Request Time p90: {latency['request_time_p90_ms']:.1f}ms")
        print(f"  Request Time p99: {latency['request_time_p99_ms']:.1f}ms")
        print(f"  API Processing p90: {latency['api_time_p90_ms']:.1f}ms")
        print(f"  Meets Target (<100ms p90): {'‚úÖ' if latency['meets_target'] else '‚ùå'}")

    if analysis.get("prediction_stats"):
        pred = analysis["prediction_stats"]
        print("\nüéØ PREDICTIONS:")
        print(f"  Average Ensemble Score: {pred['ensemble_score_mean']:.3f}")
        print(f"  Score Std Deviation: {pred['ensemble_score_std']:.3f}")
        print(f"  Average Confidence: {pred['confidence_mean']:.3f}")
        print("  Severity Distribution:")
        for severity, count in pred["severity_distribution"].items():
            percentage = (count / analysis['successful_tests'] * 100) if analysis['successful_tests'] > 0 else 0
            print(f"    {severity}: {count} ({percentage:.1f}%)")

    if analysis.get("errors"):
        print("\n‚ùå ERRORS:")
        for error in analysis["errors"][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  - {error}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
    report_path = Path(args.report)
    report_path.parent.mkdir(parents=True, exist_ok=True)

    full_report = {
        "test_config": {
            "api_base": args.api,
            "cycles_tested": args.cycles,
            "data_source": args.data,
            "batch_mode": args.batch,
            "timestamp": datetime.now(UTC).isoformat(),
        },
        "analysis": sanitize_for_json(analysis),  # ‚úÖ FIX: Safe serialization
        "raw_results": sanitize_for_json(results[:10]),  # –ü–µ—Ä–≤—ã–µ 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    }

    with report_path.open("w") as f:
        json.dump(full_report, f, indent=2)

    print(f"\nüìÑ Full report saved to: {report_path}")
    print("\n" + "=" * 60)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if analysis.get("latency_stats", {}).get("meets_target"):
        print("‚úÖ PASS: Latency requirements met!")
    else:
        print("‚ö†Ô∏è  OPTIMIZATION NEEDED: Consider model optimization or caching")

    if analysis.get("success_rate", 0) > 95:
        print("‚úÖ PASS: High success rate!")
    else:
        print("‚ö†Ô∏è  STABILITY ISSUES: Check error patterns")


if __name__ == "__main__":
    asyncio.run(main())
