# ML Inference Service Configuration
# Копировать в .env и настроить

# Application
DEBUG=false
ML_HOST=0.0.0.0
ML_PORT=8001
ML_WORKERS=4

# Models
MODEL_PATH=./models
PREDICTION_THRESHOLD=0.6
MAX_INFERENCE_TIME_MS=100
BATCH_SIZE=32

# Performance
CACHE_PREDICTIONS=true
CACHE_TTL_SECONDS=300

# Redis
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=20

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/hydraulic

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
LOG_LEVEL=INFO

# Health Checks
HEALTH_CHECK_INTERVAL=30
MODEL_WARMUP_TIMEOUT=60

# Security
ML_API_KEY=your-secret-api-key-here
CORS_ORIGINS=http://localhost:3000,http://localhost:8000

# Feature Engineering
FEATURE_WINDOW_MINUTES=10
SAMPLING_FREQUENCY_HZ=100.0

# Anomaly Detection
ANOMALY_SENSITIVITY=0.8
MIN_DATA_POINTS=100

# External Services
BACKEND_API_URL=http://localhost:8000/api
NOTIFICATION_SERVICE_URL=http://localhost:8002
