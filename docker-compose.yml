version: '3.8'

services:
  # ============================================================================
  # Database Layer
  # ============================================================================
  db:
    image: timescale/timescaledb:latest-pg15
    container_name: hdx-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${DATABASE_NAME}
      POSTGRES_USER: ${DATABASE_USER}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./docker/init-timescale.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      - internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: hdx-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    networks:
      - internal
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # ============================================================================
  # AI/ML Layer (GPU Services)
  # ============================================================================
  gnn_service:
    build:
      context: .
      dockerfile: gnn_service/Dockerfile
    image: hdx-gnn-service:latest
    container_name: hdx-gnn-service
    restart: unless-stopped
    environment:
      GNN_INTERNAL_API_KEY: ${GNN_INTERNAL_API_KEY}
      MODEL_PATH: /models/gnn_classifier_best.ckpt
      DEVICE: cuda
      CUDA_VISIBLE_DEVICES: 0
    volumes:
      - ./models/gnn:/models
      - ./logs/gnn:/logs
    networks:
      - internal  # Internal only!
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  ml_service:
    build:
      context: ./ml_service
      dockerfile: Dockerfile
    container_name: hdx-ml-service
    restart: unless-stopped
    environment:
      ML_INTERNAL_API_KEY: ${ML_INTERNAL_API_KEY}
      REDIS_URL: redis://redis:6379/0
      DEBUG: "false"
    volumes:
      - ./ml_service/models:/app/models
      - ./ml_service/data:/app/data
    networks:
      - internal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s

  rag_service:
    build:
      context: ./rag_service
      dockerfile: Dockerfile
    container_name: hdx-rag-service
    restart: unless-stopped
    environment:
      RAG_INTERNAL_API_KEY: ${RAG_INTERNAL_API_KEY}
      RAG_OLLAMA_BASE_URL: http://ollama:11434
      RAG_DEBUG: "false"
    volumes:
      - ./rag_service/data:/data
    networks:
      - internal
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s

  ollama:
    image: ollama/ollama:latest
    container_name: hdx-ollama
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - internal
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ============================================================================
  # Backend Layer (Django API Gateway)
  # ============================================================================
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: hdx-backend
    restart: unless-stopped
    env_file:
      - .env
    environment:
      DJANGO_SETTINGS_MODULE: config.settings
      ML_SERVICE_URL: http://ml_service:8001
      RAG_SERVICE_URL: http://rag_service:8002
      GNN_SERVICE_URL: http://gnn_service:8003
      ML_INTERNAL_API_KEY: ${ML_INTERNAL_API_KEY}
      RAG_INTERNAL_API_KEY: ${RAG_INTERNAL_API_KEY}
      GNN_INTERNAL_API_KEY: ${GNN_INTERNAL_API_KEY}
    volumes:
      - ./backend:/app/backend
      - static_volume:/app/backend/staticfiles
      - media_volume:/app/backend/media
    ports:
      - "8000:8000"
    networks:
      - public
      - internal
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      gnn_service:
        condition: service_healthy
      ml_service:
        condition: service_healthy
      rag_service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  # ============================================================================
  # Worker Layer (Celery)
  # ============================================================================
  celery:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: hdx-celery
    restart: unless-stopped
    command: celery -A config worker -l info
    env_file:
      - .env
    environment:
      DJANGO_SETTINGS_MODULE: config.settings
    volumes:
      - ./backend:/app/backend
    networks:
      - internal
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy

  beat:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: hdx-celery-beat
    restart: unless-stopped
    command: celery -A config beat -l info
    env_file:
      - .env
    environment:
      DJANGO_SETTINGS_MODULE: config.settings
    volumes:
      - ./backend:/app/backend
    networks:
      - internal
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy

  # ============================================================================
  # Frontend Layer (Nuxt 4)
  # ============================================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: hdx-frontend
    restart: unless-stopped
    environment:
      NUXT_PUBLIC_API_BASE_URL: http://backend:8000
      NUXT_PUBLIC_WS_URL: ws://backend:8000/ws
    ports:
      - "3000:3000"
    networks:
      - public
    depends_on:
      - backend

networks:
  public:
    driver: bridge
    name: hdx-public
  internal:
    driver: bridge
    name: hdx-internal
    internal: false  # Dev: false, Prod: true

volumes:
  pgdata:
  ollama_data:
  static_volume:
  media_volume:
