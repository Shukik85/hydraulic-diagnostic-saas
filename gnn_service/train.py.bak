"""
Training script for Temporal GAT on hydraulic diagnostics.
Multi-label classification with comprehensive metrics.
"""

import json
import logging
import time
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from config import model_config, training_config
from dataset import HydraulicGraphDataset, split_dataset
from model import TemporalGAT
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class HydraulicTrainer:
    """Trainer for hydraulic diagnostics GNN."""

    def __init__(self, config: training_config = training_config):
        self.config = config
        self.device = torch.device(config.device)
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = None

        # Training history
        self.history = {
            "train_loss": [],
            "val_loss": [],
            "train_accuracy": [],
            "val_accuracy": [],
            "component_metrics": {},
        }

        logger.info(f"Initialized trainer on device: {self.device}")

    def setup_model(self) -> TemporalGAT:
        """Initialize model, optimizer, and loss function."""
        self.model = TemporalGAT(model_config).to(self.device)

        # Multi-label loss function
        self.criterion = nn.BCEWithLogitsLoss()

        # Optimizer with weight decay
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
        )

        # Learning rate scheduler
        self.scheduler = ReduceLROnPlateau(
            self.optimizer, mode="min", factor=0.5, patience=5, verbose=True
        )

        logger.info("Model setup completed")
        return self.model

    def compute_metrics(
        self, outputs: torch.Tensor, targets: torch.Tensor
    ) -> dict[str, float]:
        """
        Compute multi-label classification metrics.

        Args:
            outputs: Model predictions [batch_size, num_classes]
            targets: Ground truth labels [batch_size, num_classes]

        Returns:
            Dictionary of metrics
        """
        # Convert logits to probabilities
        probabilities = torch.sigmoid(outputs)
        predictions = (probabilities > 0.5).float()

        # Component-wise metrics
        accuracy_per_component = []
        precision_per_component = []
        recall_per_component = []
        f1_per_component = []

        for i in range(self.model.config.num_classes):
            comp_pred = predictions[:, i]
            comp_target = targets[:, i]

            # Accuracy
            accuracy = (comp_pred == comp_target).float().mean().item()
            accuracy_per_component.append(accuracy)

            # Precision, Recall, F1 (handle division by zero)
            true_positive = (comp_pred * comp_target).sum().item()
            predicted_positive = comp_pred.sum().item()
            actual_positive = comp_target.sum().item()

            precision = true_positive / (predicted_positive + 1e-8)
            recall = true_positive / (actual_positive + 1e-8)
            f1 = 2 * precision * recall / (precision + recall + 1e-8)

            precision_per_component.append(precision)
            recall_per_component.append(recall)
            f1_per_component.append(f1)

        # Macro averages
        macro_accuracy = np.mean(accuracy_per_component)
        macro_precision = np.mean(precision_per_component)
        macro_recall = np.mean(recall_per_component)
        macro_f1 = np.mean(f1_per_component)

        # Micro averages
        micro_accuracy = (predictions == targets).float().mean().item()
        micro_precision = (predictions * targets).sum() / (predictions.sum() + 1e-8)
        micro_recall = (predictions * targets).sum() / (targets.sum() + 1e-8)
        micro_f1 = (
            2 * micro_precision * micro_recall / (micro_precision + micro_recall + 1e-8)
        )

        return {
            "accuracy_macro": macro_accuracy,
            "precision_macro": macro_precision,
            "recall_macro": macro_recall,
            "f1_macro": macro_f1,
            "accuracy_micro": micro_accuracy.item(),
            "precision_micro": micro_precision.item(),
            "recall_micro": micro_recall.item(),
            "f1_micro": micro_f1.item(),
            "component_accuracy": accuracy_per_component,
            "component_precision": precision_per_component,
            "component_recall": recall_per_component,
            "component_f1": f1_per_component,
        }

    def train_epoch(self, train_loader: DataLoader) -> tuple[float, dict]:
        """Train for one epoch."""
        self.model.train()
        total_loss = 0.0
        all_outputs = []
        all_targets = []

        for batch_idx, data in enumerate(train_loader):
            data = data.to(self.device)
            targets = data.y.to(self.device)

            self.optimizer.zero_grad()

            outputs, _, _ = self.model(data)
            loss = self.criterion(outputs, targets)

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            all_outputs.append(outputs.detach())
            all_targets.append(targets.detach())

            if batch_idx % 100 == 0:
                logger.info(
                    f"Train Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}"
                )

        # Compute metrics
        all_outputs = torch.cat(all_outputs)
        all_targets = torch.cat(all_targets)
        metrics = self.compute_metrics(all_outputs, all_targets)

        avg_loss = total_loss / len(train_loader)
        return avg_loss, metrics

    def validate_epoch(self, val_loader: DataLoader) -> tuple[float, dict]:
        """Validate for one epoch."""
        self.model.eval()
        total_loss = 0.0
        all_outputs = []
        all_targets = []

        with torch.no_grad():
            for data in val_loader:
                data = data.to(self.device)
                targets = data.y.to(self.device)

                outputs, _, _ = self.model(data)
                loss = self.criterion(outputs, targets)

                total_loss += loss.item()
                all_outputs.append(outputs)
                all_targets.append(targets)

        # Compute metrics
        all_outputs = torch.cat(all_outputs)
        all_targets = torch.cat(all_targets)
        metrics = self.compute_metrics(all_outputs, all_targets)

        avg_loss = total_loss / len(val_loader)
        return avg_loss, metrics

    def train(
        self, train_dataset: HydraulicGraphDataset, val_dataset: HydraulicGraphDataset
    ):
        """Complete training loop."""
        train_loader = train_dataset.get_data_loader()
        val_loader = val_dataset.get_data_loader(shuffle=False)

        best_val_loss = float("inf")
        patience_counter = 0

        for epoch in range(self.config.epochs):
            start_time = time.time()

            # Train epoch
            train_loss, train_metrics = self.train_epoch(train_loader)

            # Validate epoch
            val_loss, val_metrics = self.validate_epoch(val_loader)

            # Update learning rate
            self.scheduler.step(val_loss)

            # Update history
            self.history["train_loss"].append(train_loss)
            self.history["val_loss"].append(val_loss)
            self.history["train_accuracy"].append(train_metrics["accuracy_macro"])
            self.history["val_accuracy"].append(val_metrics["accuracy_macro"])

            epoch_time = time.time() - start_time

            # Log epoch results
            logger.info(
                f"Epoch {epoch + 1}/{self.config.epochs} - "
                f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                f"Train Acc: {train_metrics['accuracy_macro']:.4f}, Val Acc: {val_metrics['accuracy_macro']:.4f}, "
                f"Time: {epoch_time:.2f}s"
            )

            # Early stopping and model checkpoint
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self.save_checkpoint(epoch, val_loss, is_best=True)
                logger.info(f"New best model saved with val_loss: {val_loss:.4f}")
            else:
                patience_counter += 1
                self.save_checkpoint(epoch, val_loss, is_best=False)

            if patience_counter >= self.config.patience:
                logger.info(f"Early stopping at epoch {epoch + 1}")
                break

        # Load best model for final evaluation
        self.load_best_model()
        logger.info("Training completed")

    def save_checkpoint(self, epoch: int, val_loss: float, is_best: bool = False):
        """Save model checkpoint."""
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict(),
            "val_loss": val_loss,
            "history": self.history,
            "config": {
                "model_config": model_config.__dict__,
                "training_config": self.config.__dict__,
            },
        }

        # Save latest checkpoint
        torch.save(checkpoint, "checkpoints/gnn_classifier_latest.ckpt")

        if is_best:
            Path(self.config.model_save_path).parent.mkdir(parents=True, exist_ok=True)
            torch.save(checkpoint, self.config.model_save_path)

    def load_best_model(self):
        """Load the best model from checkpoint."""
        try:
            checkpoint = torch.load(
                self.config.model_save_path, map_location=self.device
            )
            self.model.load_state_dict(checkpoint["model_state_dict"])
            logger.info(f"Loaded best model from epoch {checkpoint['epoch']}")
        except Exception as e:
            logger.error(f"Error loading best model: {e}")

    def evaluate(self, test_dataset: HydraulicGraphDataset) -> dict:
        """Evaluate model on test set."""
        test_loader = test_dataset.get_data_loader(shuffle=False)
        test_loss, test_metrics = self.validate_epoch(test_loader)

        logger.info(
            f"Test Results - Loss: {test_loss:.4f}, Accuracy: {test_metrics['accuracy_macro']:.4f}"
        )

        # Log component-wise metrics
        for i, component in enumerate(model_config.component_names):
            logger.info(
                f"{component}: Acc={test_metrics['component_accuracy'][i]:.4f}, "
                f"Prec={test_metrics['component_precision'][i]:.4f}, "
                f"Rec={test_metrics['component_recall'][i]:.4f}, "
                f"F1={test_metrics['component_f1'][i]:.4f}"
            )

        return test_metrics


def main():
    """Main training pipeline."""
    try:
        # Load dataset
        dataset = HydraulicGraphDataset()
        train_dataset, val_dataset, test_dataset = split_dataset(dataset)

        # Initialize trainer
        trainer = HydraulicTrainer()
        trainer.setup_model()

        # Train model
        trainer.train(train_dataset, val_dataset)

        # Evaluate on test set
        test_metrics = trainer.evaluate(test_dataset)

        # Save final metrics
        with open("training_results.json", "w") as f:
            json.dump(
                {"test_metrics": test_metrics, "training_history": trainer.history},
                f,
                indent=2,
            )

        logger.info("Training pipeline completed successfully")

    except Exception as e:
        logger.error(f"Training pipeline failed: {e}")
        raise


if __name__ == "__main__":
    main()
