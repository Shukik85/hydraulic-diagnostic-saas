# RAG Service Configuration
# Копировать в .env и настроить

# =============================================================================
# INTERNAL AUTHENTICATION (REQUIRED)
# =============================================================================
# Используется для backend→rag_service аутентификации
# Генерировать: python3 -c "import secrets; print(secrets.token_urlsafe(32))"
RAG_INTERNAL_API_KEY=change-me-in-production-min-32-chars

# =============================================================================
# APPLICATION
# =============================================================================
RAG_DEBUG=false
RAG_HOST=0.0.0.0
RAG_PORT=8002

# =============================================================================
# LLM CONFIGURATION
# =============================================================================
RAG_LLM_MODEL=llama3.2:latest
RAG_OLLAMA_BASE_URL=http://ollama:11434

# =============================================================================
# EMBEDDINGS
# =============================================================================
RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
RAG_VECTOR_STORE_PATH=/data/faiss_index

# =============================================================================
# PERFORMANCE
# =============================================================================
RAG_MAX_CONTEXT_LENGTH=4096
RAG_CHUNK_SIZE=512
RAG_CHUNK_OVERLAP=50

# =============================================================================
# LOGGING
# =============================================================================
RAG_LOG_LEVEL=INFO

# =============================================================================
# NOTES:
# =============================================================================
# 1. RAG_INTERNAL_API_KEY должен совпадать с backend .env
# 2. В production установите RAG_DEBUG=false
# 3. rag_service НЕ должен быть доступен извне Docker network
# 4. Для Ollama убедитесь, что модель llama3.2 загружена
