# GNN System Integration Guide

## –û–±–∑–æ—Ä

GNN Service ‚Äî —ç—Ç–æ **"–º–æ–∑–≥ —Å–∏—Å—Ç–µ–º–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"**, –∫–æ—Ç–æ—Ä—ã–π –≤–∏–¥–∏—Ç –≥–∏–¥—Ä–∞–≤–ª–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É –∫–∞–∫ **–≥—Ä–∞—Ñ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤**.

### –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- **System-level analysis**: –ü–æ–Ω–∏–º–∞–µ—Ç –∫–∞—Å–∫–∞–¥–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã (pump ‚Üí boom ‚Üí stick)
- **Explainable AI**: Attention –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **–ø–æ—á–µ–º—É** –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∞–Ω–æ–º–∞–ª–∏—è
- **Real-time capable**: <50ms inference latency (p90)
- **Fleet management**: Batch inference –¥–ª—è –ø–∞—Ä–∫–∞ —Ç–µ—Ö–Ω–∏–∫–∏

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
–ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨ (–û–ø–µ—Ä–∞—Ç–æ—Ä/–ò–Ω–∂–µ–Ω–µ—Ä/–î–∏—Å–ø–µ—Ç—á–µ—Ä)
         ‚îÇ
         ‚îÇ UI/UX
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FRONTEND (Nuxt 4 + Digital Twin)    ‚îÇ
‚îÇ  - System Health Dashboard            ‚îÇ
‚îÇ  - Interactive Graph View             ‚îÇ
‚îÇ  - Real-time Alerts + Reasoning       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ WebSocket + REST
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   BACKEND (Django + DRF)              ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  Orchestrator Layer:                  ‚îÇ
‚îÇ  - DiagnosticCoordinator              ‚îÇ
‚îÇ  - GraphBuilder                       ‚îÇ
‚îÇ  - WebSocket Manager                  ‚îÇ
‚îÇ  - Result Aggregator                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ
    ‚Üì         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GNN   ‚îÇ  ‚îÇ Component  ‚îÇ  ‚îÇ   RAG    ‚îÇ
‚îÇ Service‚îÇ  ‚îÇ   Models   ‚îÇ  ‚îÇ Assistant‚îÇ
‚îÇ        ‚îÇ  ‚îÇ (ml_service)‚îÇ  ‚îÇ (Qwen3)  ‚îÇ
‚îÇ T-GAT  ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ          ‚îÇ
‚îÇ Attn   ‚îÇ  ‚îÇ - Pump     ‚îÇ  ‚îÇ - Docs   ‚îÇ
‚îÇ Explain‚îÇ  ‚îÇ - Cylinder ‚îÇ  ‚îÇ - History‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ            ‚îÇ            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TimescaleDB‚îÇ      ‚îÇ   Redis   ‚îÇ
‚îÇ - Sensors  ‚îÇ      ‚îÇ - Cache   ‚îÇ
‚îÇ - Topology ‚îÇ      ‚îÇ - Celery  ‚îÇ
‚îÇ - Results  ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## –°—Ü–µ–Ω–∞—Ä–∏–π A: Real-time –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –û–ø–µ—Ä–∞—Ç–æ—Ä —ç–∫—Å–∫–∞–≤–∞—Ç–æ—Ä–∞

#### –®–∞–≥ 1: –î–∞—Ç—á–∏–∫–∏ ‚Üí Backend

```python
# –ö–∞–∂–¥—ã–µ 100ms: –¥–∞–≤–ª–µ–Ω–∏–µ, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, –æ–±–æ—Ä–æ—Ç—ã, –ø–æ–ª–æ–∂–µ–Ω–∏–µ
Modbus Gateway ‚Üí Backend Ingestion API
SensorData.objects.bulk_create([
    SensorData(equipment_id=1, sensor_type="pressure", value=185.0),
    SensorData(equipment_id=1, sensor_type="temperature", value=68.0),
    ...
])
```

#### –®–∞–≥ 2: Backend Orchestrator

```python
from diagnostics.coordinator import DiagnosticCoordinator

coordinator = DiagnosticCoordinator()
result = await coordinator.run_diagnostics(
    equipment_id=1,
    mode="hybrid",  # GNN + component models
)
```

#### –®–∞–≥ 3: GNN Service Inference

**Request**:
```json
{
  "node_features": [
    [185.0, 68.0, 180.0, 2.1, ...],  // pump
    [165.0, 72.0, 120.0, 1.8, ...],  // boom
    [160.0, 70.0, 110.0, 1.5, ...],  // stick
    ...
  ],
  "edge_index": [[0, 0, 1], [1, 2, 2]],  // pump‚Üíboom, pump‚Üístick, boom‚Üístick
  "component_names": ["pump", "boom", "stick", "bucket"]
}
```

**Response (<50ms)**:
```json
{
  "prediction": 1,  // ANOMALY
  "probability": 0.94,
  "anomaly_score": 0.89,
  "explanation": {
    "critical_components": ["pump", "boom"],
    "attention_scores": [0.82, 0.67, 0.15, 0.08],
    "causal_path": [
      "pump_critical_failure",
      "boom_degradation"
    ],
    "reasoning": "Primary component affected: pump. Secondary components: boom. Causal chain detected: pump_critical_failure ‚Üí boom_degradation. System-level anomaly detected with cascading effects. Immediate inspection recommended."
  }
}
```

#### –®–∞–≥ 4: Component-level Analysis

–ï—Å–ª–∏ GNN –æ–±–Ω–∞—Ä—É–∂–∏–ª –∞–Ω–æ–º–∞–ª–∏—é, Backend –≤—ã–∑—ã–≤–∞–µ—Ç ml_service –¥–ª—è critical –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:

```python
# ml_service: pump model
result = await ml_client.predict(equipment_id=1, model_type="pump")
# {
#   "prediction": "bearing_wear",
#   "confidence": 0.95,
#   "diagnosis": "Fe particles detected in oil sample",
#   "recommended_action": "Replace bearing within 24h"
# }
```

#### –®–∞–≥ 5: Aggregation + WebSocket Push

```python
# Backend sends to frontend via WebSocket
ws_manager.broadcast(
    channel=f"equipment/{equipment_id}",
    message={
        "type": "diagnostic_alert",
        "data": {
            "system": gnn_result,
            "components": component_results,
            "recommendation": recommendation,
        }
    }
)
```

#### –®–∞–≥ 6: Frontend UI

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üî¥ SYSTEM ANOMALY (GNN)                      ‚îÇ
‚îÇ Confidence: 94% | Score: 0.89                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ üîç System Analysis:                       ‚îÇ
‚îÇ Affected: Pump ‚Üí Boom Cylinder              ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ Causal Chain:                              ‚îÇ
‚îÇ üî• Pump overheating ‚Üí üìâ Pressure drop       ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ üî¨ Component Details:                     ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ üíß Pump: Bearing wear (95% conf)         ‚îÇ
‚îÇ   Action: Replace bearing within 24h       ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ üîß Boom: Seal degradation (89% conf)      ‚îÇ
‚îÇ   Action: Inspect seals, plan repair       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚ö†Ô∏è  PRIORITY: CRITICAL                      ‚îÇ
‚îÇ üïí Timeframe: Immediate                    ‚îÇ
‚îÇ                                            ‚îÇ
‚îÇ [View Graph] [Ask AI] [Acknowledge]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## API Endpoints

### GNN Service

#### POST /predict

Single equipment inference.

**Request**:
```json
{
  "node_features": [[...]],
  "edge_index": [[...]],
  "edge_attr": [[...]] (optional),
  "component_names": ["pump", "boom", ...]
}
```

**Response**:
```json
{
  "prediction": 0 | 1,
  "probability": 0.95,
  "anomaly_score": 0.89,
  "explanation": {...} (if anomaly)
}
```

#### POST /batch_predict

Fleet batch inference.

**Request**:
```json
{
  "graphs": [
    {"equipment_id": "CAT-336-001", "node_features": [...], ...},
    {"equipment_id": "CAT-336-002", ...},
    ...
  ]
}
```

**Response**:
```json
{
  "predictions": [
    {"equipment_id": "CAT-336-001", "prediction": 1, "anomaly_score": 0.89},
    ...
  ]
}
```

#### GET /health

Health check.

#### GET /metrics

Prometheus metrics.

---

## Local Development

### Prerequisites

- Docker + Docker Compose
- NVIDIA GPU + nvidia-docker2 (–¥–ª—è GNN + Ollama)
- Python 3.11+
- Node.js 20+ (–¥–ª—è frontend)

### Quick Start

```bash
# 1. Clone repo
git clone https://github.com/Shukik85/hydraulic-diagnostic-saas.git
cd hydraulic-diagnostic-saas
git checkout feature/gnn-system-integration

# 2. Setup environment
cp .env.example .env
# Edit .env: set GNN_INTERNAL_API_KEY, ML_INTERNAL_API_KEY, RAG_INTERNAL_API_KEY

# 3. Build base GPU image
docker build -f docker/base-ai-gpu.Dockerfile -t hydraulic-ai-base-gpu:cuda12.1 .

# 4. Start services
docker-compose up -d db redis
docker-compose up -d gnn_service ml_service rag_service ollama
docker-compose up -d backend celery beat
docker-compose up -d frontend

# 5. Check health
curl http://localhost:8003/health  # GNN Service
curl http://localhost:8001/health  # ML Service
curl http://localhost:8002/health  # RAG Service
curl http://localhost:8000/health/ # Backend

# 6. Test inference
curl -X POST http://localhost:8003/predict \
  -H "X-Internal-API-Key: your-gnn-key" \
  -H "Content-Type: application/json" \
  -d '{
    "node_features": [[185.0, 68.0, 180.0, 2.1, 0, 0, 0, 0, 0, 0]],
    "edge_index": [[0], [0]],
    "component_names": ["pump"]
  }'
```

### GPU Memory Management

```bash
# Monitor GPU usage
watch -n 1 nvidia-smi

# Expected allocation:
# - GNN Service: ~2-3 GB
# - Ollama (DeepSeek-R1): ~8-10 GB
# Total: ~10-13 GB (—Ç—Ä–µ–±—É–µ—Ç—Å—è GPU —Å ‚â•16 GB VRAM)
```

---

## Production Deployment

### Kubernetes

See `k8s/gnn-service-deployment.yaml` for full configuration.

**Key points**:

- GPU node pools with NVIDIA drivers
- Horizontal Pod Autoscaler (3-10 replicas)
- PersistentVolumeClaim for model storage
- Service mesh (Istio) for traffic management
- Prometheus + Grafana for monitoring

---

## Performance Benchmarks

| Metric | Target | Status |
|--------|--------|--------|
| GNN Inference (p90) | <100ms | ‚è≥ Pending GPU testing |
| GNN Inference (p50) | <50ms | ‚è≥ Pending GPU testing |
| Batch Inference (50 graphs) | <2s | ‚è≥ Pending testing |
| Attention Explainability | <10ms | ‚úÖ Implemented |
| Model Load Time | <30s | ‚úÖ Implemented |

---

## Troubleshooting

### GNN Service won't start

**Symptom**: `Model not loaded` error

**Solution**:
1. Check model path: `MODEL_PATH=/models/gnn_classifier_best.ckpt`
2. For dev: GNN Service uses random weights if model not found
3. Check logs: `docker logs hdx-gnn-service`

### GPU not detected

**Symptom**: `CUDA not available`

**Solution**:
```bash
# Check nvidia-docker
docker run --rm --gpus all nvidia/cuda:12.1.1-base-ubuntu22.04 nvidia-smi

# Verify docker-compose GPU config
docker-compose config | grep -A5 devices
```

### High latency (>100ms)

**Symptom**: Slow inference

**Causes**:
1. Model on CPU instead of GPU
2. Large graph (>100 nodes)
3. Network overhead (Backend‚ÜíGNN)

**Solution**:
- Set `DEVICE=cuda` in .env
- Enable GPU in docker-compose
- Use batch inference for fleet

---

## Next Steps

1. **Training**: Self-Supervised GNN pretraining on UCI dataset
2. **Frontend**: Digital Twin 3D viewer + Graph View
3. **RAG Integration**: Natural language explanations
4. **K8s Deployment**: Production infrastructure

---

**Author**: Aleksandr Plotnikov  
**Date**: November 10, 2025  
**Version**: 0.1.0
