# RAG Service: DeepSeek-R1 Self-hosted

ğŸ¤– **Reasoning AI Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ GNN Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²**

## ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Diagnosis Service            â”‚
â”‚  (Orchestrator)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“ gRPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    GNN Service                  â”‚
â”‚  (ML Inference)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“ Results
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    RAG Service                  â”‚
â”‚  DeepSeek-R1-Distill-32B        â”‚
â”‚  â€¢ Reasoning Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ      â”‚
â”‚  â€¢ ĞŸĞ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ          â”‚
â”‚  â€¢ ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸

### ĞœĞ¾Ğ´ĞµĞ»ÑŒ
- **Model**: DeepSeek-R1-Distill-Qwen-32B
- **Size**: ~80GB
- **GPUs**: 2x A100 (80GB each)
- **Inference Engine**: vLLM Ñ tensor parallelism
- **Latency**: ~2-3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ per request
- **Throughput**: ~10-15 requests/minute

### Features
- âœ… Reasoning Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¾Ğ±ĞºĞ¸ (Chain-of-Thought)
- âœ… Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ GNN outputs
- âœ… ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ
- âœ… ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
- âœ… ĞŸÑ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ²
- âœ… Multi-GPU support
- âœ… Production-ready Ñ vLLM

## Quick Start

### Local Development

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Download model
python download_model.py

# 3. Run service
export TENSOR_PARALLEL_SIZE=2
python main.py
```

### Docker

```bash
# Build
docker build -t rag-service:latest .

# Run with GPU
docker run --gpus '"device=0,1"' \
  -p 8004:8004 \
  -v $(pwd)/models:/app/models \
  rag-service:latest
```

### Kubernetes

```bash
# Deploy
kubectl apply -f kubernetes/deployment.yaml

# Check status
kubectl get pods -n hydraulic-prod -l app=rag-service

# Logs
kubectl logs -f deployment/rag-service -n hydraulic-prod
```

## API Usage

### 1. Interpret GNN Diagnosis

```python
import requests

response = requests.post(
    "http://rag-service:8004/interpret/diagnosis",
    json={
        "gnn_result": {
            "overall_health_score": 0.65,
            "anomalies": [
                {
                    "anomaly_type": "pressure_drop",
                    "severity": "high",
                    "confidence": 0.85,
                    "affected_components": ["main_pump"]
                }
            ],
            "component_health": [
                {
                    "component_id": "pump_001",
                    "component_type": "Main Pump",
                    "health_score": 0.65,
                    "degradation_rate": 0.08
                }
            ]
        },
        "equipment_context": {
            "equipment_id": "exc_001",
            "equipment_type": "Excavator",
            "model": "CAT-320D",
            "manufacturer": "Caterpillar",
            "operating_hours": 8500
        }
    }
)

result = response.json()
print(result["summary"])     # ĞŸĞ¾Ğ½ÑÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ
print(result["reasoning"])   # Reasoning steps
print(result["recommendations"])  # ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ
```

### 2. Explain Anomaly

```python
response = requests.post(
    "http://rag-service:8004/explain/anomaly",
    json={
        "anomaly_type": "pressure_drop",
        "context": {
            "component": "main_pump",
            "current_pressure": 115.3,
            "normal_pressure": 150.0,
            "operating_hours": 8500
        }
    }
)

print(response.json()["explanation"])
```

### 3. Generic Generation

```python
response = requests.post(
    "http://rag-service:8004/generate",
    json={
        "prompt": "Explain hydraulic pump failure modes",
        "max_tokens": 1024,
        "temperature": 0.7
    }
)

print(response.json()["response"])
```

## Performance

### GPU Utilization

```bash
# Monitor GPU usage
watch -n 1 nvidia-smi

# Expected:
# GPU 0: ~85% utilization, ~70GB memory
# GPU 1: ~85% utilization, ~70GB memory
```

### Latency

| Request Type | Latency (p50) | Latency (p99) |
|--------------|---------------|---------------|
| Diagnosis Interpretation | 2.1s | 3.5s |
| Anomaly Explanation | 1.5s | 2.8s |
| Generic Generation | 1.8s | 3.2s |

### Throughput

- **Sequential**: ~15-20 req/min
- **Batched** (vLLM): ~40-50 req/min

## Monitoring

### Health Checks

```bash
# Health
curl http://rag-service:8004/health

# Readiness
curl http://rag-service:8004/ready
```

### Metrics

Prometheus metrics available at `/metrics`:
- `rag_requests_total` - Total requests
- `rag_request_duration_seconds` - Request latency
- `rag_gpu_utilization_percent` - GPU usage
- `rag_gpu_memory_used_bytes` - GPU memory

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_NAME` | `deepseek-ai/DeepSeek-R1-Distill-Qwen-32B` | Model ID |
| `TENSOR_PARALLEL_SIZE` | `2` | Number of GPUs |
| `GPU_MEMORY_UTIL` | `0.90` | GPU memory utilization |
| `MAX_MODEL_LEN` | `8192` | Max sequence length |
| `PORT` | `8004` | HTTP port |
| `LOG_LEVEL` | `INFO` | Logging level |

## Troubleshooting

### Model Not Loading

```bash
# Check GPU availability
nvidia-smi

# Check disk space
df -h /app/models

# Re-download model
python download_model.py
```

### OOM (Out of Memory)

```bash
# Reduce GPU memory utilization
export GPU_MEMORY_UTIL=0.85

# Reduce max sequence length
export MAX_MODEL_LEN=4096
```

### Slow Inference

```bash
# Check GPU utilization
nvidia-smi

# Enable KV cache
export VLLM_USE_KV_CACHE=1

# Increase batch size
export VLLM_MAX_NUM_SEQS=8
```

## Production Checklist

- [ ] Model downloaded and cached
- [ ] 2x A100 GPUs available
- [ ] vLLM installed correctly
- [ ] Health checks passing
- [ ] Latency < 5s p99
- [ ] GPU utilization > 70%
- [ ] Monitoring configured
- [ ] Alerts setup
- [ ] Backup inference endpoint

## License

Proprietary - Hydraulic Diagnostic SaaS
